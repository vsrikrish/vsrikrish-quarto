---
title: AI
published-title: Last updated
date: last-modified
description: How and why I use (or, don't use) AI and LLMs in my work and thinking
toc: true
toc-location: left
toc-title: Areas
freeze: false
link-citations: true
citations-hover: true
bibliography: llms.bib
csl: pnas.csl
---

This is my [/ai page](https://www.bydamo.la/p/ai-manifesto), where, in the interest of full transparency, I make explicit how I use AI in my work and what my underlying thoughts and principles are that guide that usage.

## Usage

None of the content on this website or my [lab group's website](https://viveks.bee.cornell.edu) were written, in part or in full, by a generative AI tool (more specifically, large language models, or LLMs).

I do not, and will not, intentionally write text or generate figures using LLMs (though with the increasing integration of these tools into word processors, some usage may leak in accidentally). I have occasionally (not often!) used LLMs to generate starter code for certain programming tasks (web scraping, for example), but debug and refine this code manually.

I also try to block AI bots from accessing and scraping from parsing this and any other websites I create using the associated `robots.txt`: any scraping that occurs is in violation of this access condition.

## Principles

I enjoy the act (and sometimes struggle!) of writing and programming. While LLMs can be helpful tools for certain things, they are useless (or at least awful) for others, and always involve ethical and environmental tradeoffs that should be navigated intentionally. 

I am not opposed to the *thoughtful* and *targeted* use of LLMs. These are clearly cases where they can be helpful for specific purposes in specific workflows, though I have not found any of those compelling for my own work. Contrary to the endless hype and marketing served by their creators and the media, these tools have strong limitations and over-reliance on them can greatly impede the discovery and learning processes. As Ted Chiang wrote, ["Using ChatGPT to complete assignments is like bringing a forklift into the weight room; you will never improve your cognitive fitness that way.‚Äù](https://www.newyorker.com/culture/the-weekend-essay/why-ai-isnt-going-to-make-art)[@Chiang2024-zt]

**Moreover**:

- The scalable models are provided [at a great monetary loss](https://www.wheresyoured.at/anthropic-and-openai-have-begun-the-subprime-ai-crisis/) [to their companies](https://www.wheresyoured.at/anthropic-is-bleeding-out/), which means they might be *far* more expensive (or even not available) in the future --- do you want to be dependent on them if that occurs? At the same time, [AI companies have been responsible for 60% of the stock market's growth between 2023--2025, and AI capital expenditures, as a function of GDP, have outpaced those during the dot-com boom](https://www.derekthompson.org/p/how-ai-conquered-the-us-economy-a). The combination of these factors [suggests that the U.S. economy is in the middle of an unsustainable bubble driven by AI hype](https://www.wheresyoured.at/the-haters-gui/)
- They [do not reason the way humans do](https://www.anthropic.com/research/reasoning-models-dont-say-think) and, even so-called "reasoning models" will only generate statistically-plausible text when asked to explain their output. This informs some of the thoughts on applications below: in cases where LLM output is used as a jumping off point and everything will be manually checked, this limitation is not as important, but if LLMs are being used to "answer" a question...good luck.
- Data centers used to train large-scale LLMs also have [large environmental impacts](https://news.mit.edu/2025/explained-generative-ai-environmental-impact-0117), increasing electricity and water demands and placing an increasing strain on our already-straining infrastructure systems.
- If the output of these models isn't legally plagiarism, [it isn't functionally far off](https://nickfthilton.medium.com/llms-are-definitionally-plagiaristic-fc8c00299ae3).
- ["AI", as currently discussed and deployed, is a political project](https://www.liberalcurrents.com/half-the-answer-27-ai-as-a-political-project/), and we should reckon with the implications of that project every time we use these tools or products which integrate them. That doesn't mean these tools shouldn't be used, but they should be used deliberately and with an understanding that we are engaging in a task that is, at best, ethically complicated.

## Text

I am opposed to the use of LLMs for writing other than formulaic tasks like re-formatting existing text or tweaking grammar (particularly if English is a second language)^[But I still don't like using tools for these purposes: changing grammar can change the ways sentences communicate meaning, and I am a grammatical descriptivist.]. The [process of writing is the process of thinking](https://www.latimes.com/opinion/story/2023-06-20/google-microsoft-chatgpt-ai-writing-assistants-artificial-intelligence)[@Rosenzweig2023-jh]; using LLMs to write means your ideas are likely to be half-baked and you are less likely to understand feedback because you did not think through and articulate your ideas^[As far as I can tell, much of the appeal of using LLMs to generate text is to avoid the struggle of refining ideas through writing.]. Moreover, LLMs flatten styles, even with prompt engineering: I find reading LLM-generated text to be an uninspiring slog. To paraphrase [Sabrine Zetteler](https://www.bbc.com/news/articles/c15q5qzdjqxo), why would I want to read something that the "author" couldn't bother to put effort into writing?

The fundamental problem with LLMs is that they are **bullshit generators** [@Hicks2024-tp; @Hannigan2024-cv]. Bullshit, in the philosophical sense, is text produced without care for the truth [@frankfurt_bullshit_2005]. It is not a *lie*, which specifically exists in opposition of truth, or a *mistake*, which is subject to correction when exposed to divergence from the truth, but it is agnostic to the truth; it simply exists to make the author sound like an authority. Truth simply does not matter to a bullshitter.

LLMs literally exist only to produce bullshit. They use a predictive statistical model to guess what next word (or sequence of words) is likely; there is no reference to whether the underlying idea produced by this sequence of words is truthful or even coherent^[To paraphrase [Cosma Shalizi](http://bactra.org/weblog/feral-library-card-catalogs.html), the appearance that LLMs "reason" or have insights is an [ember of autoregression](http://arxiv.org/abs/2309.13638) sprinkled with [wishful mnemonics](https://arxiv.org/abs/2104.12871)]. This means that, once an LLM goes off track, they are subject to wild hallucinations where they may invent concepts or artifacts (books, articles, etc) that do not exist, merely because they *seem plausible as a string of text*. Additionally, given that LLMs are trained on publicly available text, an increasing amount of which is now generated by LLMs (so-called **"AI slop"**), the uncritical use of these results can just perpetuate the bullshit cycle.

## Images and Video

I find the style of LLM-generated "art" to be, worse than uninteresting: it's actively off-putting and repulsive. I'm not opposed to using machine learning for targeted tasks, like color fills and object selection, is *fine*, but LLMs being shoehorned onto those tasks is unnecessary and kind of like using a sledgehammer to crack a nut.

## Research

Using LLMs as a substitute for literature reviews and idea synthesis short-cuts the learning and thinking process[@Chiang2024-zt] and [can often result](https://hai.stanford.edu/news/hallucinating-law-legal-mistakes-large-language-models-are-pervasive) [in the insertion of hallucinations](https://www.washingtonpost.com/health/2025/05/29/maha-rfk-jr-ai-garble/). The propensity of LLMs to hallucinate also fundamentally affects the reliability of LLM results for information retrieval^[As put by [computational linguist Emily Bender](https://buttondown.com/maiht3k/archive/information-literacy-and-chatbots-as-search/), "If someone uses an LLM as a replacement for search, and the output they get is correct, this is just by chance. Furthermore, a system that is right 95% of the time is arguably more dangerous tthan [*sic*] one that is right 50% of the time. People will be more likely to trust the output, and likely less able to fact check the 5%." It's worth reading the whole post about how using LLM summaries as a substitute for search harms information literacy and disrupts sense-making processes.]. LLMs can also lead to [dangerous spiraling behavior](https://www.nytimes.com/2025/08/08/technology/ai-chatbots-delusions-chatgpt.html) by encouraging incorrect ideas and thoughts. 

As always, there are much more limited applications where LLMs could be useful. I can imagine that using an LLM to give a starting set of references, **which will be read manually**, might be more useful to some people than slogging through Google Scholar^[It might also be useful to use an LLM to generate a set of search terms for Google Scholar.]. Hallucinations would therefore not be an issue: you just wouldn't find those articles, and you can expand your review by following citations and references.

## Programming

While I personally don't often use LLMs for any programming tasks^[Debugging is frustrating, but it teaches me something about the language.], I'm not opposed to their use in the same way that I am with writing. In my experience, LLM autocompletions aren't great, but they're *fine*^[Just beware of [slopsquatting](https://www.trendmicro.com/vinfo/us/security/news/cybercrime-and-digital-threats/slopsquatting-when-ai-agents-hallucinate-malicious-packages)!], and scientific programming is not necessarily focused on efficiency at all costs. Using LLMs to translate code between languages, to generate documentation, and to interpret error messages are all very reasonable.


## References